PrÃ©sentation des scripts de crawling

    sequential_crawl.py
    Ce script met en place un crawling sÃ©quentiel avec une session partagÃ©e, permettant de garder les cookies et autres donnÃ©es entre les requÃªtes. Il suit un dÃ©roulement prÃ©cis :
    ğŸ”¹ Lecture des URLs depuis un fichier Sitemap au format XML
    ğŸ”¹ Lancement dâ€™un navigateur headless pour optimiser les ressources
    ğŸ”¹ Traitement des URLs une par une, avec gÃ©nÃ©ration dâ€™un fichier Markdown par page visitÃ©e
    ğŸ”¹ Gestion des Ã©checs lors de la navigation (journalisation des erreurs)
    ğŸ”¹ Fermeture propre du navigateur une fois le processus terminÃ©

    âœ… IdÃ©al pour les cas oÃ¹ un suivi linÃ©aire et le maintien dâ€™une session sont essentiels.

    parallel_craw.py
    Ce script permet un crawling en parallÃ¨le pour amÃ©liorer considÃ©rablement la vitesse dâ€™exÃ©cution. Il suit une logique asynchrone :
    ğŸ”¹ Extraction des URLs depuis un fichier Sitemap XML
    ğŸ”¹ Initialisation dâ€™un navigateur headless comme dans le script sÃ©quentiel
    ğŸ”¹ ExÃ©cution concurrente de tÃ¢ches de crawling via asyncio
    ğŸ”¹ Pour chaque URL, crÃ©ation dâ€™un fichier Markdown documentant son contenu
    ğŸ”¹ Traitement indÃ©pendant des erreurs pour chaque tÃ¢che
    ğŸ”¹ Le navigateur est automatiquement fermÃ© grÃ¢ce Ã  lâ€™instruction async with

    âœ… RecommandÃ© pour le traitement massif et rapide de nombreuses pages.

    single_craw.py
    Version simplifiÃ©e et minimale, ce script est conÃ§u pour le crawling dâ€™une seule URL.
    ğŸ”¹ Configuration lÃ©gÃ¨re
    ğŸ”¹ Parfait pour les tests, les dÃ©mos ou les cas isolÃ©s
    ğŸ”¹ GÃ©nÃ©re Ã©galement un fichier Markdown rÃ©sumant le contenu de la page
